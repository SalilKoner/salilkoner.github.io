\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,color,float}
\usepackage{graphicx,psfrag,epsf}
\usepackage{natbib}


\setlength{\oddsidemargin}{.15in} 
\setlength{\textwidth}{6.25in}
\setlength{\topmargin}{-0.25in}
\setlength{\headheight}{-0.15in}
\setlength{\textheight}{8.9in} 

\linespread{1.25}


\title{ST 705 Linear models and variance components \\ 
        Homework problem set 1}


\begin{document}
\maketitle

\begin{enumerate}

\item Prove the following theorem.  Let $V$ be a vector space and $B = \{u_{1},\dots,u_{n}\}$ be a subset of $V$.  Then $B$ is a basis if and only if each $v \in V$ can be expressed \textit{uniquely} as 
\[
v = a_{1}u_{1} + \cdots + a_{n}u_{n}
\] 
for some set of scalars $\{a_{1},\dots,a_{n}\}$.

\item Prove that the eigenvalues of an upper triangular matrix $M$ are the diagonal components of $M$.  Write pseudo-code for an algorithm that exploits this fact for more efficiently computing the matrix exponential $e^{M}$ (recall the definition of the matrix exponential from your linear algebra course).

\item The defining property of a projection matrix $A$ is that $A^{2} = A$ (recall the definition of the square of a matrix from your linear algebra course).  Establish the following facts.
\begin{enumerate}
\item If $A$ is a projection matrix, then all of its eigenvalues are either zero or one. 
\item If $A \in \mathbb{R}^{p\times p}$ is a projection and symmetric (i.e., an orthogonal projection matrix), then for every vector $v$ the projection $Av$ is orthogonal to $v - Av$.
\item $\text{tr}(A + B) = \text{tr}(A) + \text{tr}(B)$.
\item $\text{tr}(AB) = \text{tr}(BA)$.
\end{enumerate}

\item Let $A \in \mathbb{R}^{p\times p}$ be symmetric.  Use the spectral decomposition of $A$ to show that 
\[
\sup_{x\in\mathbb{R}^{p}\setminus\{0\}} \frac{x'Ax}{x'x} = \lambda_{\max},
\]
where $\lambda_{\max}$ is the largest eigenvalue of $A$.  Observe that this is a special case of the Courant-Fischer theorem (see \verb1https://en.wikipedia.org/wiki/Min-max_theorem1).

\item Let $x = (x_{1}, \dots, x_{p})' \in \mathbb{R}^{p}$.  Show that for $i \in \{1,\dots,p\}$,
\[
|x_{i}| \le \|x\|_{2} \le \|x\|_{1},
\]
where $\|\cdot\|_{1}$ and $\|\cdot\|_{2}$ are the $l_{1}$ and $l_{2}$ vector norms, respectively.

\item Show that every eigenvalue of a real symmetric matrix is real.

\item Show that if $X \sim \text{N}_{p}(\mu, \Sigma)$ and $Y = X'AX$, then $E(Y) = \text{tr}(A\Sigma) + \mu'A\mu$.

\item Let $U$ and $V$ be random variables.  Establish the following inequalities.
\begin{enumerate}
\item $P(|U+V| > a + b) \le P(|U| > a) + P(|V| > b)$ for every $a,b \ge 0$.
\item $P(|UV| > a) \le P(|U| > a/b) + P(|V| > b)$ for every $a \ge 0$ and $b > 0$.
\end{enumerate}

\end{enumerate}






\end{document}